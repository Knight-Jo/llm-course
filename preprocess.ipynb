{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d19ec5-ae34-46f4-b4ee-b250bf42f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mikasenghaas/wikitext-2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/nas511/guandewei/.cache/huggingface/datasets/mikasenghaas___wikitext-2/default/0.0.0/f7836bd4080d244e6507fcf70604c740d73a230a (last modified on Sun May 11 17:42:42 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mikasenghaas/wikitext-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a3267a-f7cd-4230-82e0-91e2144530bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据：删除空行和标题行（以 \" = \" 开头）\n",
    "# def clean_wikitext(examples):\n",
    "#     cleaned_texts = []\n",
    "#     for text in examples[\"text\"]:\n",
    "#         if text.strip() and not text.startswith(\" = \"):\n",
    "#             cleaned_texts.append(text)\n",
    "#     return {\"text\": cleaned_texts}\n",
    "import re\n",
    "import ftfy  # 用于修复常见的Unicode问题\n",
    "def clean_wikitext(examples):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for text in examples[\"text\"]:\n",
    "        # 1. 基础清洗 删除空行\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # 2. 修复Unicode字符问题（使用ftfy库）\n",
    "        text = ftfy.fix_text(text)\n",
    "        \n",
    "        # 3. 过滤维基百科结构化内容\n",
    "        # 匹配标题行（包含不同层级的标题 = Title == Subtitle === Section ====）\n",
    "        if re.match(r'^\\s*=+\\s.*\\s=+\\s*$', text):\n",
    "            continue\n",
    "        # 过滤列表项、导航模板\n",
    "        if text.startswith(('* ', '# ', '{{', '}}', '|-')):\n",
    "            continue\n",
    "        # 过滤文件链接和分类标记\n",
    "        if re.search(r'\\[\\[(File|Category):', text):\n",
    "            continue\n",
    "            \n",
    "        # 4. 清理维基标记语法\n",
    "        # 移除内部链接标记（保留链接文字）\n",
    "        text = re.sub(r'\\[\\[([^\\]|]+)\\|?([^\\]]+)?\\]\\]', lambda m: m.group(2) or m.group(1), text)\n",
    "        # 移除模板\n",
    "        text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "        # 移除引用标记\n",
    "        text = re.sub(r'<ref.*?</ref>', '', text, flags=re.DOTALL)\n",
    "        \n",
    "        # 5. 文本规范化\n",
    "        # 合并多个换行/空格\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # 移除特殊字符（保留常见标点）\n",
    "        text = re.sub(r'[^\\w\\s.,!?\\'\"-—–@$%&*+/:;()]', '', text)\n",
    "        # 标准化引号\n",
    "        text = text.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        \n",
    "        # 6. 内容质量过滤\n",
    "        # 过滤过短/无意义的句子\n",
    "        if len(text) < 25:\n",
    "            continue\n",
    "        # 过滤纯数字内容\n",
    "        if re.fullmatch(r'\\d+[\\d,\\.%\\s]*', text):\n",
    "            continue\n",
    "        \n",
    "        cleaned_texts.append(text)\n",
    "    \n",
    "    return {\"text\": cleaned_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ad5f41-b19e-460c-92cf-039b0b74370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用清洗\n",
    "train_data = ds[\"train\"].map(clean_wikitext, batched=True)\n",
    "val_data = ds[\"validation\"].map(clean_wikitext, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec49ac5b-fc8f-4792-aef7-2085ae7a300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5045fc-ba18-46ba-86f4-203da83c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e1fcf7-1c4b-45a9-88a5-143781549b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词与格式化（输入为前 256 tokens，目标为后 256 tokens）\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # 分割输入和目标（前 256 tokens 为输入，后 256 tokens 为目标）\n",
    "    inputs = {k: v[:, :256] for k, v in tokenized.items()}\n",
    "    labels = {k: v[:, 256:] for k, v in tokenized.items()}\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1927f07-a815-4b18-a51e-53d24a4df8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5aa1f6967e410f908ae1832bd17fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff0914e147f43348180bc63a2e86094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 应用分词\n",
    "tokenized_train = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = val_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54f6fb6-5808-4363-a9ef-9911c59ac186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 16958\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ee036-e177-440b-babe-f03f970d2c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ffa42-067c-4b5f-ac47-05fcbd1ae840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d8c29e5-75ec-4e0d-8756-3eab534799bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74144a7e50f14a67a4ad9c617d59cf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "local_model_path = \"./Qwen3-1.7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"balanced_low_0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2933fb7e-7762-4de4-952c-56abb491a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1759990/1234338071.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='20620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  145/20620 04:17 < 10:13:49, 0.56 it/s, Epoch 0.07/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.841400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 25\u001b[0m\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./qwen3-wikitext-epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 根据验证损失选择最佳模型\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(         \n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./qwen3-wikitext-sft-epoch-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-lr-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/transformers/trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/accelerate/accelerator.py:2450\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "epochs = 10\n",
    "lr = 5e-6\n",
    "batch_size=8\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./qwen3-wikitext-epoch{epochs}\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_steps=50,\n",
    "    learning_rate=lr,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,  # 减少显存压力\n",
    "    weight_decay=0.01,  # 添加权重衰减\n",
    "    metric_for_best_model=\"eval_loss\",  # 根据验证损失选择最佳模型\n",
    ")\n",
    "\n",
    "trainer = Trainer(         \n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(f\"./qwen3-wikitext-sft-epoch-{epochs}-lr-{lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40140d5-0b2f-46cc-a9d4-a0ab5a13dcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.config.update({'ServerApp': {'iopub_data_rate_limit': 100000000}})</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<script>Jupyter.notebook.config.update({'ServerApp': {'iopub_data_rate_limit': 100000000}})</script>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9b35b2-cd17-4a94-8775-3dffc3b51752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7428ffaaad0b45e69d04f4b476092124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
      "[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 398.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 71.61it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.36it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.25s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.90it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.45it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.14it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.37s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.30it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.03it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.68it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.37it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.40it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 定义评估任务（使用 wikitext 的困惑度）\n",
    "tasks = [\"wikitext\"]\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_model(model_path):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 包装为 lm_eval 兼容的模型\n",
    "    lm = HFLM(\n",
    "        pretrained=model,  # 直接传入已加载的模型\n",
    "        tokenizer=tokenizer,\n",
    "        device=\"cuda\"      # 指定设备\n",
    "    )\n",
    "    \n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=lm,\n",
    "        tasks=tasks,\n",
    "        batch_size=2,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    #print(f\"results: {results}\")\n",
    "    #return results[\"results\"][\"wikitext\"][\"perplexity\"]\n",
    "    return results\n",
    "# # 微调前评估\n",
    "base_rst = evaluate_model(\"./Qwen3-1.7B\")\n",
    "print(f\"微调前困惑度: {base_rst}\")\n",
    "# # 微调后评估 \n",
    "#finetuned_rst = evaluate_model(\"./qwen3-wikitext-sft\")\n",
    "#print(f\"微调后困惑度: {finetuned_rst}\")\n",
    "\n",
    "# # 微调前评估\n",
    "# base_ppl = evaluate_model(\"./Qwen3-1.7B\")\n",
    "# # 微调后评估\n",
    "# finetuned_ppl = evaluate_model(\"./qwen3-wikitext-sft\")\n",
    "\n",
    "# print(f\"微调前困惑度: {base_ppl:.2f}\")\n",
    "# print(f\"微调后困惑度: {finetuned_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff933e3-e35c-48c4-80e8-69c2ee78c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4e15306b2c46ae9c9c4b42dc77d9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
      "[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "\n",
      "  0%|                                                                                                                          | 0/62 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 327.28it/s]\u001b[A\n",
      "\n",
      "  0%|                                                                                                                          | 0/62 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████████████▊                                                                                                     | 7/62 [00:00<00:01, 52.79it/s]\u001b[A\n",
      " 23%|█████████████████████████▌                                                                                       | 14/62 [00:00<00:00, 60.17it/s]\u001b[A\n",
      " 39%|███████████████████████████████████████████▋                                                                     | 24/62 [00:00<00:00, 68.56it/s]\u001b[A\n",
      " 53%|████████████████████████████████████████████████████████████▏                                                    | 33/62 [00:00<00:00, 71.40it/s]\u001b[A\n",
      " 66%|██████████████████████████████████████████████████████████████████████████▋                                      | 41/62 [00:00<00:00, 56.30it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 70.54it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.46it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.76it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.24it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.73it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.34s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.82it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.56s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.92it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.35it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.83s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.68it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.04it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.04it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.82it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.26s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.16it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.70it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.12it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.69s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.79it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.28s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.77it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.32it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.34it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.06s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.97it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.75s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running loglikelihood requests: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.46it/s]\u001b[A\n",
      "\n",
      "Running loglikelihood requests:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.84 GiB. GPU 1 has a total capacity of 23.64 GiB of which 6.08 GiB is free. Process 493197 has 854.00 MiB memory in use. Including non-PyTorch memory, this process has 16.70 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m finetuned_rst \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./qwen3-wikitext-sft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m微调后困惑度: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinetuned_rst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 包装为 lm_eval 兼容的模型\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lm \u001b[38;5;241m=\u001b[39m HFLM(\n\u001b[1;32m     16\u001b[0m     pretrained\u001b[38;5;241m=\u001b[39mmodel,  \u001b[38;5;66;03m# 直接传入已加载的模型\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     18\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m      \u001b[38;5;66;03m# 指定设备\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#print(f\"results: {results}\")\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#return results[\"results\"][\"wikitext\"][\"perplexity\"]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/home/nas928/guandewei/okto/llm-course/lm-evaluation-harness/lm_eval/utils.py:439\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/nas928/guandewei/okto/llm-course/lm-evaluation-harness/lm_eval/evaluator.py:338\u001b[0m, in \u001b[0;36msimple_evaluate\u001b[0;34m(model, model_args, tasks, num_fewshot, batch_size, max_batch_size, device, use_cache, cache_requests, rewrite_requests_cache, delete_requests_cache, limit, samples, bootstrap_iters, check_integrity, write_out, log_samples, evaluation_tracker, system_instruction, apply_chat_template, fewshot_as_multiturn, gen_kwargs, task_manager, verbosity, predict_only, random_seed, numpy_random_seed, torch_random_seed, fewshot_random_seed, confirm_run_unsafe_code, metadata)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluation_tracker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     evaluation_tracker\u001b[38;5;241m.\u001b[39mgeneral_config_tracker\u001b[38;5;241m.\u001b[39mlog_experiment_args(\n\u001b[1;32m    329\u001b[0m         model_source\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    330\u001b[0m         model_args\u001b[38;5;241m=\u001b[39mmodel_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m         fewshot_as_multiturn\u001b[38;5;241m=\u001b[39mfewshot_as_multiturn,\n\u001b[1;32m    336\u001b[0m     )\n\u001b[0;32m--> 338\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_requests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_requests\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewrite_requests_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewrite_requests_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbootstrap_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpredict_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlog_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_instruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_as_multiturn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfewshot_as_multiturn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfirm_run_unsafe_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfirm_run_unsafe_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     setup_logging(verbosity\u001b[38;5;241m=\u001b[39mverbosity)\n",
      "File \u001b[0;32m/home/nas928/guandewei/okto/llm-course/lm-evaluation-harness/lm_eval/utils.py:439\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/nas928/guandewei/okto/llm-course/lm-evaluation-harness/lm_eval/evaluator.py:570\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(lm, task_dict, limit, samples, cache_requests, rewrite_requests_cache, bootstrap_iters, write_out, log_samples, system_instruction, apply_chat_template, fewshot_as_multiturn, verbosity, confirm_run_unsafe_code)\u001b[0m\n\u001b[1;32m    567\u001b[0m         cloned_reqs\u001b[38;5;241m.\u001b[39mextend([req] \u001b[38;5;241m*\u001b[39m req\u001b[38;5;241m.\u001b[39mrepeats)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# run requests through model\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m resps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloned_reqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# put responses from model into a list of length K for each request.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, cloned_reqs):\n",
      "File \u001b[0;32m/home/nas928/guandewei/okto/llm-course/lm-evaluation-harness/lm_eval/models/huggingface.py:999\u001b[0m, in \u001b[0;36mHFLM.loglikelihood_rolling\u001b[0;34m(self, requests, disable_tqdm)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m# Extract just the windows for processing, keeping track of request indices\u001b[39;00m\n\u001b[1;32m    997\u001b[0m batch_indices, batch_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m--> 999\u001b[0m batch_nlls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loglikelihood_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_bs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_windows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# Store results with their request indices\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m all_nlls\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mzip\u001b[39m(batch_indices, batch_nlls))\n",
      "File \u001b[0;32m/home/nas928/guandewei/okto/llm-course/lm-evaluation-harness/lm_eval/models/huggingface.py:1210\u001b[0m, in \u001b[0;36mHFLM._loglikelihood_tokens\u001b[0;34m(self, requests, disable_tqdm, override_bs)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     batched_encoder_mask \u001b[38;5;241m=\u001b[39m pad_and_concat(\n\u001b[1;32m   1203\u001b[0m         padding_len_inp, encoder_attns\n\u001b[1;32m   1204\u001b[0m     )  \u001b[38;5;66;03m# [batch, padding_len_inp]\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m     call_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: batched_encoder_mask,\n\u001b[1;32m   1207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: batched_conts,\n\u001b[1;32m   1208\u001b[0m     }\n\u001b[0;32m-> 1210\u001b[0m multi_logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, padding_length (inp or cont), vocab]\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (request_str, ctx_tokens, _), logits, inplen, cont_toks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m   1217\u001b[0m     chunk, multi_logits, inplens, cont_toks_list\n\u001b[1;32m   1218\u001b[0m ):\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# Slice to original seq length\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     contlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cont_toks)\n",
      "File \u001b[0;32m~/.conda/envs/llm-course/lib/python3.10/site-packages/torch/nn/functional.py:2248\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2250\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.84 GiB. GPU 1 has a total capacity of 23.64 GiB of which 6.08 GiB is free. Process 493197 has 854.00 MiB memory in use. Including non-PyTorch memory, this process has 16.70 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "finetuned_rst = evaluate_model(\"./qwen3-wikitext-sft\")\n",
    "print(f\"微调后困惑度: {finetuned_rst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc72f90-97c7-40d9-9b33-a72f1e3b91cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
